{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# Function to parse a TML file (TimeML format) and extract events, T-LINKs, and TIMEX3\n",
        "def parse_tml_with_context(file_path):\n",
        "    tree = ET.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    events = []\n",
        "    tlinks = []\n",
        "    timex3s = []\n",
        "\n",
        "    # Extract events and TIMEX3\n",
        "    for s in root.iter('TEXT'):\n",
        "        sentence_text = s.text\n",
        "        for event in s.iter('EVENT'):\n",
        "            event_id = event.attrib['eid']\n",
        "            event_text = event.text\n",
        "            events.append({'EVENT ID': event_id, 'EVENT Text': event_text, 'Context Sentence': sentence_text})\n",
        "\n",
        "        for timex in s.iter('TIMEX3'):\n",
        "            timex_id = timex.attrib['tid']\n",
        "            timex_text = timex.text\n",
        "            timex3s.append({'TIMEX3 ID': timex_id, 'TIMEX3 Text': timex_text})\n",
        "\n",
        "    # Extract T-LINKs\n",
        "    for tlink in root.iter('TLINK'):\n",
        "        event_id_1 = tlink.attrib.get('eventInstanceID')\n",
        "        event_id_2 = tlink.attrib.get('relatedToEventInstance')\n",
        "        relation = tlink.attrib.get('relType')\n",
        "\n",
        "        if event_id_1 and event_id_2:\n",
        "            tlinks.append({'Event ID 1': event_id_1, 'Event ID 2': event_id_2, 'Relation': relation})\n",
        "\n",
        "    events_df = pd.DataFrame(events)\n",
        "    timex3_df = pd.DataFrame(timex3s)\n",
        "    tlinks_df = pd.DataFrame(tlinks)\n",
        "\n",
        "    return events_df, timex3_df, tlinks_df\n",
        "\n",
        "# Load the datasets\n",
        "timebank_events_df, timebank_timex3_df, timebank_tlinks_df = parse_tml_with_context('TimeBank.tml')\n",
        "timeeval3_events_df, timeeval3_timex3_df, timeeval3_tlinks_df = parse_tml_with_context('TimeEval3.tml')\n",
        "\n",
        "# Combine the datasets\n",
        "combined_events_df = pd.concat([timebank_events_df, timeeval3_events_df], ignore_index=True)\n",
        "combined_timex3_df = pd.concat([timebank_timex3_df, timeeval3_timex3_df], ignore_index=True)\n",
        "combined_tlinks_df = pd.concat([timebank_tlinks_df, timeeval3_tlinks_df], ignore_index=True)\n",
        "\n",
        "# Prepare input data for events\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(combined_events_df['EVENT Text'].tolist())  # Fit on event texts\n",
        "event_sequences = tokenizer.texts_to_sequences(combined_events_df['EVENT Text'].tolist())  # Convert texts to sequences\n",
        "\n",
        "# Pad event sequences\n",
        "padded_event_sequences = pad_sequences(event_sequences, padding='post', dtype='int32')  # Pad event sequences\n",
        "\n",
        "# Create encoded labels\n",
        "relation_mapping = {relation: idx for idx, relation in enumerate(combined_tlinks_df['Relation'].unique())}\n",
        "encoded_labels = []\n",
        "for index, row in combined_events_df.iterrows():\n",
        "    # Find the corresponding relation for the event\n",
        "    relation = combined_tlinks_df[\n",
        "        (combined_tlinks_df['Event ID 1'] == row['EVENT ID']) |\n",
        "        (combined_tlinks_df['Event ID 2'] == row['EVENT ID'])\n",
        "    ]['Relation']\n",
        "\n",
        "    if not relation.empty:\n",
        "        encoded_labels.append(relation_mapping[relation.values[0]])\n",
        "    else:\n",
        "        # Assign a valid class index for \"no relation\"\n",
        "        encoded_labels.append(len(relation_mapping))  # Adjust if needed\n",
        "\n",
        "# Convert to a numpy array\n",
        "encoded_labels = np.array(encoded_labels)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(padded_event_sequences, encoded_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define model parameters\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Size of the vocabulary\n",
        "embedding_dim = 128  # Dimension of the embedding layer\n",
        "max_length = X_train.shape[1]  # Maximum length of the input sequences\n",
        "num_classes = len(relation_mapping) + 1  # Number of classes including \"no relation\"\n",
        "\n",
        "# Build the LSTM model with Bidirectional LSTM for better performance\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True, kernel_regularizer=l2(0.001))))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Bidirectional(LSTM(64, kernel_regularizer=l2(0.001))))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax', kernel_regularizer=l2(0.001)))  # +1 for \"no relation\" class\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
        "\n",
        "# Function to prepare event sequences for a given sentence\n",
        "def prepare_event_sequences(events, tokenizer, maxlen):\n",
        "    # Convert events to sequences\n",
        "    event_sequences = tokenizer.texts_to_sequences(events)\n",
        "    # Pad sequences\n",
        "    padded_sequences = pad_sequences(event_sequences, padding='post', maxlen=maxlen)\n",
        "    return padded_sequences\n",
        "\n",
        "# Function to order events based on model predictions\n",
        "def predict_event_order(sentence, model, tokenizer, maxlen):\n",
        "    # Simple event extraction from the input sentence (for demonstration purposes)\n",
        "    events = sentence.split(\", \")  # You can implement more sophisticated extraction methods\n",
        "\n",
        "    # Tokenize and pad the event sequences\n",
        "    event_sequences = prepare_event_sequences(events, tokenizer, maxlen)\n",
        "\n",
        "    # Make predictions (probabilities for each class)\n",
        "    predictions = model.predict(event_sequences)\n",
        "\n",
        "    # Get the predicted order by selecting the highest probability for each event\n",
        "    predicted_order = np.argsort(np.argmax(predictions, axis=1))  # Sort by predicted indices\n",
        "\n",
        "    # Return events in the predicted order\n",
        "    ordered_events = [events[i] for i in predicted_order]\n",
        "    return ordered_events\n",
        "\n",
        "# Define a sample sentence with unordered events\n",
        "sample_sentence = \"Alice woke up, made breakfast, attended a meeting, and then went for a run.\"\n",
        "\n",
        "# Predict and print the ordered events\n",
        "ordered_events = predict_event_order(sample_sentence, model, tokenizer, max_length)\n",
        "\n",
        "print(\"\\nOriginal Events:\")\n",
        "print(sample_sentence)\n",
        "\n",
        "print(\"\\nPredicted Events in Correct Temporal Order:\")\n",
        "for event in ordered_events:\n",
        "    print(f\"- {event}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aURSU1wEqd-8",
        "outputId": "bad0ba24-e0ac-4bb7-ee97-58fb7e86ff6b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 797ms/step - accuracy: 0.2339 - loss: 2.6373 - val_accuracy: 1.0000 - val_loss: 2.5976\n",
            "Epoch 2/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 2.5946 - val_accuracy: 1.0000 - val_loss: 2.5560\n",
            "Epoch 3/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 2.5526 - val_accuracy: 1.0000 - val_loss: 2.5152\n",
            "Epoch 4/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 2.5120 - val_accuracy: 1.0000 - val_loss: 2.4749\n",
            "Epoch 5/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 2.4701 - val_accuracy: 1.0000 - val_loss: 2.4350\n",
            "Epoch 6/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 2.4316 - val_accuracy: 1.0000 - val_loss: 2.3953\n",
            "Epoch 7/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 2.3926 - val_accuracy: 1.0000 - val_loss: 2.3557\n",
            "Epoch 8/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 2.3516 - val_accuracy: 1.0000 - val_loss: 2.3159\n",
            "Epoch 9/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 2.3106 - val_accuracy: 1.0000 - val_loss: 2.2757\n",
            "Epoch 10/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 2.2688 - val_accuracy: 1.0000 - val_loss: 2.2346\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 568ms/step\n",
            "\n",
            "Original Events:\n",
            "Alice woke up, made breakfast, attended a meeting, and then went for a run.\n",
            "\n",
            "Predicted Events in Correct Temporal Order:\n",
            "- Alice woke up\n",
            "- made breakfast\n",
            "- attended a meeting\n",
            "- and then went for a run.\n"
          ]
        }
      ]
    }
  ]
}