{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oz_jNdj6Wb4M",
        "outputId": "cb3f8c7c-d1ec-462b-f115-47643ff7f118"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing file: MATRES.csv\n",
            "Index(['_unit_id', '_golden', '_unit_state', '_trusted_judgments',\n",
            "       '_last_judgment_at',\n",
            "       'can_the_verb_span_stylecolorblueverb_span_be_anchored_in_time',\n",
            "       'can_the_verb_span_stylecolorblueverb_span_be_anchored_in_time:confidence',\n",
            "       'after', 'before', 'bodytext',\n",
            "       'can_the_verb_span_stylecolorblueverb_span_be_anchored_in_time_gold',\n",
            "       'docid', 'eventid', 'verb'],\n",
            "      dtype='object')\n",
            "Extracted 1188 sentences from MATRES.csv\n",
            "Generating embeddings for 1188 sentences\n",
            "Generated embeddings shape: (1188, 768)\n",
            "Extracting temporal cues for 1188 sentences\n",
            "Relevance scores: [0, 0, 0, 0, 0]\n",
            "Epoch 1/10\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6661 - loss: 0.6283 - val_accuracy: 0.7353 - val_loss: 0.5681\n",
            "Epoch 2/10\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7649 - loss: 0.5542 - val_accuracy: 0.7353 - val_loss: 0.5458\n",
            "Epoch 3/10\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7563 - loss: 0.5502 - val_accuracy: 0.7353 - val_loss: 0.5300\n",
            "Epoch 4/10\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7677 - loss: 0.5187 - val_accuracy: 0.7353 - val_loss: 0.5153\n",
            "Epoch 5/10\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7716 - loss: 0.4938 - val_accuracy: 0.7353 - val_loss: 0.5032\n",
            "Epoch 6/10\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7841 - loss: 0.4642 - val_accuracy: 0.7437 - val_loss: 0.5051\n",
            "Epoch 7/10\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7878 - loss: 0.4666 - val_accuracy: 0.7689 - val_loss: 0.4765\n",
            "Epoch 8/10\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8060 - loss: 0.4345 - val_accuracy: 0.7605 - val_loss: 0.4738\n",
            "Epoch 9/10\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8182 - loss: 0.4348 - val_accuracy: 0.7605 - val_loss: 0.4641\n",
            "Epoch 10/10\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8325 - loss: 0.3903 - val_accuracy: 0.7647 - val_loss: 0.4668\n",
            "Generating embeddings for 1 sentences\n",
            "Generated embeddings shape: (1, 768)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "Predicted relevance score (1: relevant, 0: not relevant): 1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, models\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Initialize BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# SECTION 1: Data Extraction\n",
        "def extract_sentences_from_matres(matres_file):\n",
        "    \"\"\"Extract sentences from the MATRES dataset.\"\"\"\n",
        "    print(f\"Processing file: {matres_file}\")\n",
        "    matres_data = pd.read_csv(matres_file)  # Load the CSV file\n",
        "\n",
        "    print(matres_data.columns)  # Display column names for debugging\n",
        "\n",
        "    # Using 'bodytext' as the column containing the sentences\n",
        "    sentences_column = 'bodytext'  # Ensure this is the correct column\n",
        "    sentences = matres_data[sentences_column].dropna().tolist()  # Drop NaN values\n",
        "\n",
        "    print(f\"Extracted {len(sentences)} sentences from {matres_file}\")\n",
        "\n",
        "    return sentences\n",
        "\n",
        "# SECTION 2: Embedding Generation\n",
        "def generate_embeddings(sentences):\n",
        "    \"\"\"Generate embeddings using BERT.\"\"\"\n",
        "    print(f\"Generating embeddings for {len(sentences)} sentences\")\n",
        "    embeddings = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "        with torch.no_grad():  # Disable gradient calculations\n",
        "            outputs = bert_model(**inputs)\n",
        "            # Use the mean of the last hidden states as the embedding\n",
        "            sentence_embedding = outputs.last_hidden_state.mean(dim=1).numpy()\n",
        "            embeddings.append(sentence_embedding)\n",
        "\n",
        "    # Convert to numpy array\n",
        "    embeddings = np.vstack(embeddings)\n",
        "    print(f\"Generated embeddings shape: {embeddings.shape}\")\n",
        "    return embeddings\n",
        "\n",
        "# SECTION 3: Temporal Cue Extraction\n",
        "def extract_temporal_cues(sentences, temporal_keywords):\n",
        "    \"\"\"Extract temporal cues based on specified keywords.\"\"\"\n",
        "    print(f\"Extracting temporal cues for {len(sentences)} sentences\")\n",
        "    relevance_scores = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Check for the presence of temporal keywords\n",
        "        if any(keyword in sentence for keyword in temporal_keywords):\n",
        "            relevance_scores.append(1)  # Relevant\n",
        "        else:\n",
        "            relevance_scores.append(0)  # Not relevant\n",
        "\n",
        "    print(f\"Relevance scores: {relevance_scores[:5]}\")  # Print sample of relevance scores\n",
        "    return np.array(relevance_scores)\n",
        "\n",
        "# SECTION 4: Main Workflow\n",
        "def main(matres_file, temporal_keywords):\n",
        "    # Step 1: Load sentences from MATRES\n",
        "    matres_sentences = extract_sentences_from_matres(matres_file)\n",
        "\n",
        "    # Step 2: Generate embeddings\n",
        "    embeddings = generate_embeddings(matres_sentences)\n",
        "\n",
        "    # Step 3: Extract relevance scores\n",
        "    relevance_scores = extract_temporal_cues(matres_sentences, temporal_keywords)\n",
        "\n",
        "    # Step 4: Prepare the data for training\n",
        "    X = np.array(embeddings)  # Input embeddings\n",
        "    y = np.array(relevance_scores)  # Binary temporal relevance labels (1 or 0)\n",
        "\n",
        "    # Step 5: Split into training and validation datasets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Step 6: Build the LSTM model\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Input(shape=(X.shape[1],)))\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
        "\n",
        "    # Step 7: Compile the model\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Step 8: Train the model\n",
        "    model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
        "\n",
        "    # Step 9: Test the model with a random sentence\n",
        "    test_sentence = \"The meeting will take place after lunch.\"\n",
        "    test_embedding = generate_embeddings([test_sentence])  # Generate actual embedding for the test sentence\n",
        "    predicted_relevance = model.predict(test_embedding)\n",
        "    print(f\"Predicted relevance score (1: relevant, 0: not relevant): {1 if predicted_relevance[0][0] > 0.5 else 0}\")\n",
        "\n",
        "# SECTION 5: Execution\n",
        "if __name__ == \"__main__\":\n",
        "    matres_file = 'MATRES.csv'  # Path to the MATRES CSV file\n",
        "    temporal_keywords = ['after', 'before', 'during', 'until', 'while', 'as soon as', 'when']  # Keywords for cue extraction\n",
        "    main(matres_file, temporal_keywords)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}