{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preparation:**\n",
        "\n",
        "Loaded and cleaned the MC-TACO dataset, including renaming columns, converting the \"stationarity\" column to a binary target, and removing any NaN values.\n",
        "Embedding Generation:\n",
        "\n",
        "Used DistilBERT to generate embeddings (vector representations) for the event descriptions, which are used as inputs to the model.\n",
        "\n",
        "**LSTM Model Creation:**\n",
        "\n",
        "Defined an LSTM model for classifying the event descriptions into two categories (stationarity vs. event duration).\n",
        "\n",
        "\n",
        "**Model Training:**\n",
        "\n",
        "Trained the LSTM model on the DistilBERT embeddings, using a custom training loop with cross-entropy loss and the Adam optimizer.\n",
        "\n",
        "**Prediction:**\n",
        "\n",
        "Applied the trained model to make predictions for a sample sentence, using DistilBERT embeddings and LSTM output to classify the event."
      ],
      "metadata": {
        "id": "95gt7Y3N7YiI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What We Should Accomplish:**\n",
        "\n",
        "**Train the LSTM Model**: Use the event embeddings and stationarity labels to train the LSTM model. The model should learn to predict whether an event is stationary or of variable duration.\n",
        "\n",
        "**Make Predictions**: Once the model is trained, you should be able to input new sentences and the model will classify them as either:\n",
        "\n",
        "Stationarity (1)\n",
        "Event Duration (0)\n",
        "\n",
        "**Evaluate the Model**: Youâ€™ll assess its accuracy, precision, recall, and F1-score to measure how well it performs on unseen test data."
      ],
      "metadata": {
        "id": "dykovAZ68BL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Z2bioKGg6S_",
        "outputId": "293dcd5d-7cf6-445c-9a43-3e40515fb20f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import RandomOverSampler  # Import RandomOverSampler\n",
        "\n",
        "# Load the data from the TSV file\n",
        "data = pd.read_csv('mc-taco.tsv', sep='\\t')\n",
        "\n",
        "# Clean up column names and rename for easier access\n",
        "data.columns = data.columns.str.strip().str.replace(' ', '_')\n",
        "data.rename(columns={\n",
        "    'Islam_later_emerged_as_the_majority_religion_during_the_centuries_of_Ottoman_rule,_though_a_significant_Christian_minority_remained.': 'event_description',\n",
        "    'Stationarity': 'stationarity'\n",
        "}, inplace=True)\n",
        "\n",
        "# Filter relevant columns\n",
        "filtered_data = data[['event_description', 'stationarity']]\n",
        "\n",
        "# Convert the Stationarity column to binary\n",
        "filtered_data['stationarity'] = filtered_data['stationarity'].map({\n",
        "    'Stationarity': 1,\n",
        "    'Event Duration': 0,\n",
        "    'Frequency': 0,\n",
        "    'Event Ordering': 0,\n",
        "    'Typical Time': 0\n",
        "})\n",
        "\n",
        "# Remove rows with NaN values in the 'stationarity' column\n",
        "filtered_data = filtered_data.dropna(subset=['stationarity'])\n",
        "\n",
        "# Check class distribution\n",
        "print(\"Class distribution before oversampling:\")\n",
        "print(filtered_data['stationarity'].value_counts())\n",
        "\n",
        "# Prepare the target variable\n",
        "y = filtered_data['stationarity'].values\n",
        "\n",
        "# Initialize DistilBERT tokenizer and model\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "distilbert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Function to convert a sentence into its DistilBERT representation\n",
        "def get_sentence_vector(sentence):\n",
        "    inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = distilbert_model(**inputs)\n",
        "    # Use the output of the last hidden state\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "\n",
        "# Create embeddings for the entire dataset\n",
        "X = np.array([get_sentence_vector(desc) for desc in filtered_data['event_description']])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "# Apply Random Oversampling to the training set\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
        "\n",
        "# Convert to tensors\n",
        "X_train = torch.tensor(X_train_resampled, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train_resampled, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Define the LSTM model\n",
        "class MyLSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MyLSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # Add sequence dimension\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        out = self.fc(lstm_out[:, -1, :])  # Get the last output from LSTM\n",
        "        return self.sigmoid(out)  # Output probability\n",
        "\n",
        "# Initialize the model\n",
        "input_size = X_train.shape[1]  # Number of features from DistilBERT\n",
        "hidden_size = 128  # Choose a hidden size\n",
        "lstm_model = MyLSTMModel(input_size, hidden_size)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
        "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training the model\n",
        "def train_model(model, X_train, y_train, criterion, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        outputs = model(X_train)  # Forward pass\n",
        "        loss = criterion(outputs.squeeze(), y_train)  # Calculate loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update parameters\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Train the model\n",
        "train_model(lstm_model, X_train, y_train, criterion, optimizer, epochs=10)\n",
        "\n",
        "# Function to predict the stationarity of a new sentence\n",
        "def predict_stationarity(model, new_sentences):\n",
        "    # Convert new sentences to DistilBERT embeddings\n",
        "    new_X = np.array([get_sentence_vector(sentence) for sentence in new_sentences])\n",
        "    new_X_tensor = torch.tensor(new_X, dtype=torch.float32)\n",
        "\n",
        "    # Make predictions\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(new_X_tensor)\n",
        "    predicted_labels = (predictions > 0.5).float()  # Threshold at 0.5 for binary classification\n",
        "    return predicted_labels.numpy()\n",
        "\n",
        "# Make predictions on the test set\n",
        "lstm_model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = lstm_model(X_test)\n",
        "    predicted_labels = (test_outputs > 0.5).float().numpy()  # Convert probabilities to binary labels\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test.numpy(), predicted_labels)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Classification report for detailed metrics\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test.numpy(), predicted_labels, target_names=['Not Stationarity (0)', 'Stationarity (1)']))\n",
        "\n",
        "# Example inputs, including one that belongs to stationarity\n",
        "example_inputs = [\n",
        "    \"The event was planned to occur next week.\",  # Not Stationary\n",
        "    \"The meeting will take place regularly every Monday.\",  # Not Stationary\n",
        "    \"This festival is held only once a year.\",  # Not Stationary\n",
        "    \"The concert will be held every year without fail.\",  # Not Stationary\n",
        "    \"This workshop happens every spring.\",  # Not Stationary\n",
        "    \"The exhibition is scheduled to occur next month.\"  # Stationary\n",
        "]\n",
        "\n",
        "# Make predictions on the example inputs\n",
        "predicted_stationarity = predict_stationarity(lstm_model, example_inputs)\n",
        "\n",
        "# Display the predictions in binary format (0 or 1)\n",
        "for sentence, prediction in zip(example_inputs, predicted_stationarity):\n",
        "    print(f\"Sentence: '{sentence}' => Prediction: {int(prediction[0])}\")  # Convert to int for binary output\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZR38cKRj73s",
        "outputId": "f1cd1588-ec55-4b4a-dd90-fb6f569e9a72"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-790f8e5bef4b>:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_data['stationarity'] = filtered_data['stationarity'].map({\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution before oversampling:\n",
            "stationarity\n",
            "0    3510\n",
            "1     272\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.6934\n",
            "Epoch [2/10], Loss: 0.6887\n",
            "Epoch [3/10], Loss: 0.6843\n",
            "Epoch [4/10], Loss: 0.6795\n",
            "Epoch [5/10], Loss: 0.6746\n",
            "Epoch [6/10], Loss: 0.6697\n",
            "Epoch [7/10], Loss: 0.6644\n",
            "Epoch [8/10], Loss: 0.6588\n",
            "Epoch [9/10], Loss: 0.6530\n",
            "Epoch [10/10], Loss: 0.6470\n",
            "Test Accuracy: 0.6486\n",
            "\n",
            "Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "Not Stationarity (0)       0.94      0.66      0.78       703\n",
            "    Stationarity (1)       0.09      0.44      0.15        54\n",
            "\n",
            "            accuracy                           0.65       757\n",
            "           macro avg       0.52      0.55      0.47       757\n",
            "        weighted avg       0.88      0.65      0.73       757\n",
            "\n",
            "Sentence: 'The event was planned to occur next week.' => Prediction: 0\n",
            "Sentence: 'The meeting will take place regularly every Monday.' => Prediction: 0\n",
            "Sentence: 'This festival is held only once a year.' => Prediction: 0\n",
            "Sentence: 'The concert will be held every year without fail.' => Prediction: 0\n",
            "Sentence: 'This workshop happens every spring.' => Prediction: 0\n",
            "Sentence: 'The exhibition is scheduled to occur next month.' => Prediction: 0\n"
          ]
        }
      ]
    }
  ]
}