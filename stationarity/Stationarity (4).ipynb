{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preparation:**\n",
        "\n",
        "Loaded and cleaned the MC-TACO dataset, including renaming columns, converting the \"stationarity\" column to a binary target, and removing any NaN values.\n",
        "Embedding Generation:\n",
        "\n",
        "Used DistilBERT to generate embeddings (vector representations) for the event descriptions, which are used as inputs to the model.\n",
        "\n",
        "**LSTM Model Creation:**\n",
        "\n",
        "Defined an LSTM model for classifying the event descriptions into two categories (stationarity vs. event duration).\n",
        "\n",
        "\n",
        "**Model Training:**\n",
        "\n",
        "Trained the LSTM model on the DistilBERT embeddings, using a custom training loop with cross-entropy loss and the Adam optimizer.\n",
        "\n",
        "**Prediction:**\n",
        "\n",
        "Applied the trained model to make predictions for a sample sentence, using DistilBERT embeddings and LSTM output to classify the event."
      ],
      "metadata": {
        "id": "95gt7Y3N7YiI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What We Should Accomplish:**\n",
        "\n",
        "**Train the LSTM Model**: Use the event embeddings and stationarity labels to train the LSTM model. The model should learn to predict whether an event is stationary or of variable duration.\n",
        "\n",
        "**Make Predictions**: Once the model is trained, you should be able to input new sentences and the model will classify them as either:\n",
        "\n",
        "Stationarity (1)\n",
        "Event Duration (0)\n",
        "\n",
        "**Evaluate the Model**: Youâ€™ll assess its accuracy, precision, recall, and F1-score to measure how well it performs on unseen test data."
      ],
      "metadata": {
        "id": "dykovAZ68BL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Load the data from the TSV file\n",
        "data = pd.read_csv('mc-taco.tsv', sep='\\t')\n",
        "\n",
        "# Clean up column names and rename for easier access\n",
        "data.columns = data.columns.str.strip().str.replace(' ', '_')\n",
        "data.rename(columns={\n",
        "    'Islam_later_emerged_as_the_majority_religion_during_the_centuries_of_Ottoman_rule,_though_a_significant_Christian_minority_remained.': 'event_description',\n",
        "    'Stationarity': 'stationarity'\n",
        "}, inplace=True)\n",
        "\n",
        "# Filter relevant columns\n",
        "filtered_data = data[['event_description', 'stationarity']]\n",
        "\n",
        "# Map Stationarity to 1 and Event Duration to 0\n",
        "filtered_data['stationarity'] = filtered_data['stationarity'].map({\n",
        "    'Stationarity': 1,\n",
        "    'Event Duration': 0\n",
        "})\n",
        "\n",
        "# Drop rows that are not related to Stationarity or Event Duration\n",
        "filtered_data = filtered_data.dropna(subset=['stationarity'])\n",
        "\n",
        "# Check class distribution\n",
        "print(\"Class distribution before oversampling:\")\n",
        "print(filtered_data['stationarity'].value_counts())\n",
        "\n",
        "# Prepare the target variable\n",
        "y = filtered_data['stationarity'].values\n",
        "\n",
        "# Initialize DistilBERT tokenizer and model\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "distilbert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Function to convert a sentence into its DistilBERT representation\n",
        "def get_sentence_vector(sentence):\n",
        "    inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = distilbert_model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "\n",
        "# Create embeddings for the entire dataset\n",
        "X = np.array([get_sentence_vector(desc) for desc in filtered_data['event_description']])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "# Apply SMOTE to the training set\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Convert to tensors\n",
        "X_train = torch.tensor(X_train_resampled, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train_resampled, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Define the enhanced LSTM model\n",
        "class MyEnhancedLSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MyEnhancedLSTMModel, self).__init__()\n",
        "        self.lstm1 = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True)\n",
        "        self.lstm2 = nn.LSTM(hidden_size * 2, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.5)  # Adding dropout layer to prevent overfitting\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # Add sequence dimension\n",
        "        lstm_out, _ = self.lstm1(x)\n",
        "        lstm_out, _ = self.lstm2(lstm_out)\n",
        "        out = self.fc(self.dropout(lstm_out[:, -1, :]))  # Get the last output from LSTM\n",
        "        return self.sigmoid(out)  # Output probability\n",
        "\n",
        "# Initialize the enhanced model\n",
        "input_size = X_train.shape[1]  # Number of features from DistilBERT\n",
        "hidden_size = 128  # Choose a hidden size\n",
        "lstm_model = MyEnhancedLSTMModel(input_size, hidden_size)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
        "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training the model\n",
        "def train_model(model, X_train, y_train, criterion, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        outputs = model(X_train)  # Forward pass\n",
        "        loss = criterion(outputs.squeeze(), y_train)  # Calculate loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update parameters\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Train the enhanced model\n",
        "train_model(lstm_model, X_train, y_train, criterion, optimizer, epochs=10)\n",
        "\n",
        "# Make predictions on the test set\n",
        "lstm_model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = lstm_model(X_test)\n",
        "    predicted_labels = (test_outputs > 0.5).float().numpy()  # Adjust threshold as necessary\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test.numpy(), predicted_labels)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Classification report for detailed metrics\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test.numpy(), predicted_labels, target_names=['Event Duration (0)', 'Stationarity (1)']))\n",
        "\n",
        "# Example sentences for prediction\n",
        "example_sentences = [\n",
        "    \"The company held its annual meeting in the same venue as last year.\",\n",
        "    \"The train arrived at the station at 6 PM and remained there until the next morning.\",\n",
        "    \"The sun rises in the east every day, as it always has.\",\n",
        "    \"They lived in the city for almost a decade before moving to the countryside.\",\n",
        "    \"The concert started promptly at 8 PM and ended two hours later.\",\n",
        "    \"The water in the lake has remained still for several days.\",\n",
        "    \"He studied for his exams for three continuous hours last night.\",\n",
        "    \"The museum opens every morning at 9 AM and closes at 5 PM.\",\n",
        "    \"The bird perched on the branch, watching the surroundings for a long time.\",\n",
        "    \"The basketball game lasted for two intense hours before the final whistle.\",\n",
        "    \"The monument has stood in the city center for over a century.\",\n",
        "    \"The conference lasted for three days, concluding with a keynote speech.\",\n",
        "    \"The cat slept in the sun all afternoon.\",\n",
        "    \"The factory operates 24 hours a day, with multiple shifts.\",\n",
        "    \"The cake baked in the oven for 30 minutes before being removed.\",\n",
        "    \"The car remained parked on the street all night.\",\n",
        "    \"The meeting was scheduled to last one hour but extended to two.\",\n",
        "    \"The moon has orbited the Earth for billions of years.\",\n",
        "    \"The dog ran around the park for nearly an hour before stopping.\",\n",
        "    \"The rain fell continuously for two days without any breaks.\",\n",
        "    \"The clock has hung on the wall since the house was built.\",\n",
        "    \"The flowers bloomed in the garden for the entire summer.\",\n",
        "    \"The construction of the new building took two years to complete.\",\n",
        "    \"The river has flowed through the valley for centuries.\",\n",
        "    \"The party lasted until midnight, with music and dancing.\",\n",
        "    \"The road remained closed for three days due to heavy snowfall.\",\n",
        "    \"The plane flew across the Atlantic in eight hours.\",\n",
        "    \"The fire burned in the fireplace for several hours before dying out.\",\n",
        "    \"The store remains open 24/7 throughout the entire year.\",\n",
        "    \"The professor lectured for 90 minutes without taking a break.\",\n",
        "    \"The stars have shined in the night sky for as long as anyone can remember.\",\n",
        "    \"The bus ride took an hour to reach its destination.\",\n",
        "    \"The boat drifted along the river for days without any direction.\",\n",
        "    \"The seminar continued for three hours, followed by a Q&A session.\",\n",
        "    \"The statue has stood in the plaza for over a hundred years.\",\n",
        "    \"The engine ran continuously for five hours before shutting down.\",\n",
        "    \"The ice cream melted within minutes after being left in the sun.\",\n",
        "    \"The tree grew slowly over the course of many decades.\",\n",
        "    \"The soccer match lasted 90 minutes, with extra time added.\",\n",
        "    \"The phone remained on the desk for several days without being touched.\"\n",
        "]\n",
        "\n",
        "# Function to predict the stationarity of a new sentence\n",
        "def predict_stationarity(model, new_sentences):\n",
        "    # Convert new sentences to DistilBERT embeddings\n",
        "    new_X = np.array([get_sentence_vector(sentence) for sentence in new_sentences])\n",
        "    new_X_tensor = torch.tensor(new_X, dtype=torch.float32)\n",
        "\n",
        "    # Make predictions\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        predictions = model(new_X_tensor)\n",
        "\n",
        "    # Apply threshold to get binary predictions\n",
        "    predicted_labels = (predictions > 0.5).float().numpy()  # Using threshold of 0.5\n",
        "\n",
        "    return predicted_labels\n",
        "\n",
        "# Call the function to predict for the example sentences\n",
        "predictions = predict_stationarity(lstm_model, example_sentences)\n",
        "\n",
        "# Display predictions\n",
        "for sentence, prediction in zip(example_sentences, predictions):\n",
        "    label = 'Stationarity (1)' if prediction == 1 else 'Event Duration (0)'  # Output labels\n",
        "    print(f\"Sentence: \\\"{sentence}\\\" -> Prediction: {label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sGr37NbF1Wf",
        "outputId": "49a6ef0c-c7c3-4a36-c785-4c36d9463ad3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-c765687e8939>:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_data['stationarity'] = filtered_data['stationarity'].map({\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution before oversampling:\n",
            "stationarity\n",
            "0.0    1112\n",
            "1.0     272\n",
            "Name: count, dtype: int64\n",
            "Epoch [1/10], Loss: 0.6943\n",
            "Epoch [2/10], Loss: 0.6917\n",
            "Epoch [3/10], Loss: 0.6898\n",
            "Epoch [4/10], Loss: 0.6875\n",
            "Epoch [5/10], Loss: 0.6849\n",
            "Epoch [6/10], Loss: 0.6817\n",
            "Epoch [7/10], Loss: 0.6765\n",
            "Epoch [8/10], Loss: 0.6706\n",
            "Epoch [9/10], Loss: 0.6638\n",
            "Epoch [10/10], Loss: 0.6559\n",
            "Test Accuracy: 0.6101\n",
            "\n",
            "Classification Report:\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "Event Duration (0)       0.86      0.61      0.72       223\n",
            "  Stationarity (1)       0.27      0.59      0.37        54\n",
            "\n",
            "          accuracy                           0.61       277\n",
            "         macro avg       0.57      0.60      0.54       277\n",
            "      weighted avg       0.75      0.61      0.65       277\n",
            "\n",
            "Sentence: \"The company held its annual meeting in the same venue as last year.\" -> Prediction: Event Duration (0)\n",
            "Sentence: \"The train arrived at the station at 6 PM and remained there until the next morning.\" -> Prediction: Stationarity (1)\n",
            "Sentence: \"The sun rises in the east every day, as it always has.\" -> Prediction: Event Duration (0)\n",
            "Sentence: \"They lived in the city for almost a decade before moving to the countryside.\" -> Prediction: Event Duration (0)\n",
            "Sentence: \"The concert started promptly at 8 PM and ended two hours later.\" -> Prediction: Event Duration (0)\n",
            "Sentence: \"The water in the lake has remained still for several days.\" -> Prediction: Stationarity (1)\n",
            "Sentence: \"He studied for his exams for three continuous hours last night.\" -> Prediction: Event Duration (0)\n",
            "Sentence: \"The museum opens every morning at 9 AM and closes at 5 PM.\" -> Prediction: Event Duration (0)\n",
            "Sentence: \"The bird perched on the branch, watching the surroundings for a long time.\" -> Prediction: Event Duration (0)\n",
            "Sentence: \"The basketball game lasted for two intense hours before the final whistle.\" -> Prediction: Event Duration (0)\n",
            "Sentence: \"The monument has stood in the city center for over a century.\" -> Prediction: Stationarity (1)\n",
            "Sentence: \"The conference lasted for three days, concluding with a keynote speech.\" -> Prediction: Event Duration (0)\n",
            "Sentence: \"The cat slept in the sun all afternoon.\" -> Prediction: Event Duration (0)\n",
            "Sentence: \"The factory operates 24 hours a day, with multiple shifts.\" -> Prediction: Event Duration (0)\n",
            "Sentence: \"The cake baked in the oven for 30 minutes before being removed.\" -> Prediction: Event Duration (0)\n",
            "Sentence: \"The car remained parked on the street all night.\" -> Prediction: Event Duration (0)\n",
            "Sentence: \"The meeting was scheduled to last one hour but extended to two.\" -> Prediction: Event Duration (0)\n",
            "Sentence: \"The moon has orbited the Earth for billions of years.\" -> Prediction: Stationarity (1)\n",
            "Sentence: \"The dog ran around the park for nearly an hour before stopping.\" -> Prediction: Event Duration (0)\n",
            "Sentence: \"The rain fell continuously for two days without any breaks.\" -> Prediction: Event Duration (0)\n",
            "Sentence: \"The clock has hung on the wall since the house was built.\" -> Prediction: Stationarity (1)\n",
            "Sentence: \"The flowers bloomed in the garden for the entire summer.\" -> Prediction: Event Duration (0)\n",
            "Sentence: \"The construction of the new building took two years to complete.\" -> Prediction: Event Duration (0)\n",
            "Sentence: \"The river has flowed through the valley for centuries.\" -> Prediction: Stationarity (1)\n",
            "Sentence: \"The party lasted until midnight, with music and dancing.\" -> Prediction: Event Duration (0)\n",
            "Sentence: \"The road remained closed for three days due to heavy snowfall.\" -> Prediction: Event Duration (0)\n",
            "Sentence: \"The plane flew across the Atlantic in eight hours.\" -> Prediction: Event Duration (0)\n",
            "Sentence: \"The fire burned in the fireplace for several hours before dying out.\" -> Prediction: Event Duration (0)\n",
            "Sentence: \"The store remains open 24/7 throughout the entire year.\" -> Prediction: Event Duration (0)\n",
            "Sentence: \"The professor lectured for 90 minutes without taking a break.\" -> Prediction: Event Duration (0)\n",
            "Sentence: \"The stars have shined in the night sky for as long as anyone can remember.\" -> Prediction: Stationarity (1)\n",
            "Sentence: \"The bus ride took an hour to reach its destination.\" -> Prediction: Event Duration (0)\n",
            "Sentence: \"The boat drifted along the river for days without any direction.\" -> Prediction: Event Duration (0)\n",
            "Sentence: \"The seminar continued for three hours, followed by a Q&A session.\" -> Prediction: Event Duration (0)\n",
            "Sentence: \"The statue has stood in the plaza for over a hundred years.\" -> Prediction: Stationarity (1)\n",
            "Sentence: \"The engine ran continuously for five hours before shutting down.\" -> Prediction: Event Duration (0)\n",
            "Sentence: \"The ice cream melted within minutes after being left in the sun.\" -> Prediction: Event Duration (0)\n",
            "Sentence: \"The tree grew slowly over the course of many decades.\" -> Prediction: Event Duration (0)\n",
            "Sentence: \"The soccer match lasted 90 minutes, with extra time added.\" -> Prediction: Event Duration (0)\n",
            "Sentence: \"The phone remained on the desk for several days without being touched.\" -> Prediction: Event Duration (0)\n"
          ]
        }
      ]
    }
  ]
}