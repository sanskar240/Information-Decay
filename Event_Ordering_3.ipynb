{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJc5oV6Iy-Fe"
      },
      "source": [
        "# Problem Formulation\n",
        "\n",
        "Instead of predicting whether one event occurs before another (binary classification), the goal is to predict the entire sequence of events in the correct chronological order. This shifts the focus from pairwise comparisons to sequence prediction.\n",
        "\n",
        "#Inputs and Outputs\n",
        "\n",
        "Inputs: A sequence of events, represented by their textual descriptions. Each event within a context (e.g., a sentence or paragraph) is tokenized and represented as a sequence of embeddings.\n",
        "\n",
        "Outputs: The output is a sequence representing the correct chronological order of these events. For example, if three events are given, the model might output a permutation of indices like [2, 0, 1], indicating the correct order.\n",
        "\n",
        "#The Architecture\n",
        "\n",
        "1. Input Representation\n",
        "The input to the model is a sequence of events, where each event is represented as a tokenized sequence of words.\n",
        "\n",
        "Event Sequence: Each event is turned into a sequence of tokens (e.g., words or subwords).\n",
        "Contextual Information: The context of each event (e.g., the surrounding sentence) is also tokenized and can be included to provide additional information.\n",
        "2. Embedding Layer\n",
        "\n",
        "The tokenized sequences are passed through an Embedding Layer. This layer converts each token (word) into a dense vector representation, capturing semantic meanings and relationships between words.\n",
        "The result is a sequence of embeddings representing the events and their contexts.\n",
        "\n",
        "3. Sequence Processing (LSTM/GRU)\n",
        "The embedded event sequences are fed into LSTM (Long Short-Term Memory) or GRU (Gated Recurrent Unit) layers.\n",
        "These layers process the sequence of embeddings, capturing temporal dependencies and the order in which events occur.\n",
        "Since LSTMs can remember long-term dependencies, they are well-suited for understanding the sequence and temporal relationships between events.\n",
        "\n",
        "4. Attention Mechanism (Optional)\n",
        "An Attention Mechanism can be applied to focus on the most important parts of the input sequence when making predictions.\n",
        "Attention helps the model weigh the significance of each event in the sequence, making it easier to determine their correct order.\n",
        "\n",
        "5. Output Layer: Sequence Prediction\n",
        "After processing the sequence with LSTM/GRU layers, the model generates an output that predicts the order of events.\n",
        "The output layer typically consists of dense layers with a softmax activation function, which outputs a probability distribution over the possible event orders.\n",
        "\n",
        "6. Decoding the Output\n",
        "Training: During training, the model is trained to minimize the difference between its predicted order and the correct order (ground truth). The correct order comes from the temporal relations (T-LINKs) in the dataset.\n",
        "Inference: During inference, the model outputs a sequence of indices that represent the predicted chronological order of the events.\n",
        "\n",
        "7. Example\n",
        "Consider a sequence of three events:\n",
        "\n",
        "Input Sequence: [\"Event A happened\", \"Event B happened\", \"Event C happened\"]\n",
        "\n",
        "Tokenized Input: [[1, 34, 56], [2, 34, 57], [3, 34, 58]] (where numbers are token indices)\n",
        "\n",
        "Model Prediction: The model processes this sequence and outputs [2, 1, 3], meaning the correct temporal order is Event B, Event A, Event C.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7lu0EDX394i"
      },
      "source": [
        "# 1.Extraction\n",
        "Whats going on here?\n",
        "\n",
        "1. Parsing TimeML Files\n",
        "\n",
        "2. Extracting Events and Context Sentences\n",
        "\n",
        "The events are stored in a list of dictionaries, where each dictionary contains:\n",
        "\n",
        "EVENT ID: The unique identifier for the event.\n",
        "\n",
        "EVENT Text: The text content of the event.\n",
        "\n",
        "Context Sentence: The sentence in which the event appears.\n",
        "\n",
        "3. Extracting Temporal Links (T-LINKs):\n",
        "\n",
        "The T-LINKs are stored in a list of dictionaries, where each dictionary contains:\n",
        "\n",
        "Event ID 1: The first event in the temporal relationship.\n",
        "\n",
        "Event ID 2: The second event in the temporal relationship.\n",
        "\n",
        "Relation: The type of temporal relation between the two events.\n",
        "\n",
        "\n",
        "4. Combining DataFrames\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Parsing and Combining Datasets\n",
        "\n",
        "Function Definition: A function (parse_tml_with_context) is created to parse TimeML files and extract:\n",
        "\n",
        "Events: Including their IDs, text, and the context sentence they belong to.\n",
        "\n",
        "TIMEX3: Temporal expressions along with their IDs.\n",
        "\n",
        "T-LINKs: Relationships between events.\n",
        "\n",
        "Dataset Loading: The function is called for two datasets, TimeBank.tml and TimeEval3.tml, producing three DataFrames for each dataset (events, TIMEX3, T-LINKs).\n",
        "\n",
        "Combining DataFrames: The resulting DataFrames from both datasets are concatenated to form combined DataFrames for events, TIMEX3, and T-LINKs.\n",
        "\n",
        "Output: The first few rows of each combined DataFrame are printed for inspection."
      ],
      "metadata": {
        "id": "61CaffxPDbbG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Input Data"
      ],
      "metadata": {
        "id": "SrncZOFw8Cqj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input Data Preparation\n",
        "\n",
        "\n",
        "Padded Sequences of Event Texts:\n",
        "\n",
        "This will be the primary input for the LSTM model.\n",
        "You will tokenize the text of events and pad the sequences to ensure uniform input length.\n",
        "\n",
        "Encoded Labels from T-LINKs:\n",
        "\n",
        "These will serve as the target output for the model.\n",
        "You will encode the relationships specified by the T-LINKs into numerical labels.\n",
        "\n",
        "TIMEX3 Information (Optional):\n",
        "\n",
        "If you choose to include TIMEX3 entities, this could provide additional temporal context.\n",
        "You can extract TIMEX3 texts and either use them as additional features or include them in the context of the event texts."
      ],
      "metadata": {
        "id": "42pYVQOb8qbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# Function to parse a TML file (TimeML format) and extract events, T-LINKs, and TIMEX3\n",
        "def parse_tml_with_context(file_path):\n",
        "    tree = ET.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    events = []\n",
        "    tlinks = []\n",
        "    timex3s = []\n",
        "\n",
        "    # Extract events and TIMEX3\n",
        "    for s in root.iter('TEXT'):\n",
        "        sentence_text = s.text\n",
        "        for event in s.iter('EVENT'):\n",
        "            event_id = event.attrib['eid']\n",
        "            event_text = event.text\n",
        "            events.append({'EVENT ID': event_id, 'EVENT Text': event_text, 'Context Sentence': sentence_text})\n",
        "\n",
        "        for timex in s.iter('TIMEX3'):\n",
        "            timex_id = timex.attrib['tid']\n",
        "            timex_text = timex.text\n",
        "            timex3s.append({'TIMEX3 ID': timex_id, 'TIMEX3 Text': timex_text})\n",
        "\n",
        "    # Extract T-LINKs\n",
        "    for tlink in root.iter('TLINK'):\n",
        "        event_id_1 = tlink.attrib.get('eventInstanceID')\n",
        "        event_id_2 = tlink.attrib.get('relatedToEventInstance')\n",
        "        relation = tlink.attrib.get('relType')\n",
        "\n",
        "        if event_id_1 and event_id_2:\n",
        "            tlinks.append({'Event ID 1': event_id_1, 'Event ID 2': event_id_2, 'Relation': relation})\n",
        "\n",
        "    events_df = pd.DataFrame(events)\n",
        "    timex3_df = pd.DataFrame(timex3s)\n",
        "    tlinks_df = pd.DataFrame(tlinks)\n",
        "\n",
        "    return events_df, timex3_df, tlinks_df\n",
        "\n",
        "# Load the datasets\n",
        "timebank_events_df, timebank_timex3_df, timebank_tlinks_df = parse_tml_with_context('TimeBank.tml')\n",
        "timeeval3_events_df, timeeval3_timex3_df, timeeval3_tlinks_df = parse_tml_with_context('TimeEval3.tml')\n",
        "\n",
        "# Combine the datasets\n",
        "combined_events_df = pd.concat([timebank_events_df, timeeval3_events_df], ignore_index=True)\n",
        "combined_timex3_df = pd.concat([timebank_timex3_df, timeeval3_timex3_df], ignore_index=True)\n",
        "combined_tlinks_df = pd.concat([timebank_tlinks_df, timeeval3_tlinks_df], ignore_index=True)\n",
        "\n",
        "# Prepare input data for events\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(combined_events_df['EVENT Text'].tolist())  # Fit on event texts\n",
        "event_sequences = tokenizer.texts_to_sequences(combined_events_df['EVENT Text'].tolist())  # Convert texts to sequences\n",
        "\n",
        "# Prepare input data for TIMEX3 (if present)\n",
        "if not combined_timex3_df.empty:\n",
        "    timex3_sequences = tokenizer.texts_to_sequences(combined_timex3_df['TIMEX3 Text'].tolist())\n",
        "else:\n",
        "    timex3_sequences = []\n",
        "\n",
        "# Pad event sequences\n",
        "padded_event_sequences = pad_sequences(event_sequences, padding='post', dtype='int32')  # Pad event sequences\n",
        "\n",
        "# Pad TIMEX3 sequences if they exist\n",
        "if timex3_sequences:\n",
        "    padded_timex3_sequences = pad_sequences(timex3_sequences, padding='post', dtype='int32')  # Pad TIMEX3 sequences\n",
        "else:\n",
        "    padded_timex3_sequences = np.empty((padded_event_sequences.shape[0], 0), dtype='int32')  # Create empty array\n",
        "\n",
        "# Combine event and TIMEX3 sequences only if TIMEX3 sequences exist\n",
        "if padded_timex3_sequences.size > 0:\n",
        "    combined_input_sequences = np.concatenate((padded_event_sequences, padded_timex3_sequences), axis=1)\n",
        "else:\n",
        "    combined_input_sequences = padded_event_sequences  # Use only event sequences\n",
        "\n",
        "# Display the DataFrames\n",
        "print(\"Events DataFrame:\")\n",
        "print(combined_events_df.head())  # Display first few rows of events\n",
        "\n",
        "print(\"\\nTIMEX3 DataFrame:\")\n",
        "print(combined_timex3_df.head())  # Display first few rows of TIMEX3\n",
        "\n",
        "print(\"\\nT-LINKs DataFrame:\")\n",
        "print(combined_tlinks_df.head())  # Display first few rows of T-LINKs\n",
        "\n",
        "# Create encoded labels\n",
        "relation_mapping = {relation: idx for idx, relation in enumerate(combined_tlinks_df['Relation'].unique())}\n",
        "encoded_labels = []\n",
        "for index, row in combined_events_df.iterrows():\n",
        "    # Find the corresponding relation for the event\n",
        "    relation = combined_tlinks_df[\n",
        "        (combined_tlinks_df['Event ID 1'] == row['EVENT ID']) |\n",
        "        (combined_tlinks_df['Event ID 2'] == row['EVENT ID'])\n",
        "    ]['Relation']\n",
        "\n",
        "    if not relation.empty:\n",
        "        encoded_labels.append(relation_mapping[relation.values[0]])\n",
        "    else:\n",
        "        # Assign a valid class index for \"no relation\"\n",
        "        encoded_labels.append(len(relation_mapping))  # Adjust if needed\n",
        "\n",
        "# Convert to a numpy array\n",
        "encoded_labels = np.array(encoded_labels)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(combined_input_sequences, encoded_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training Set Shape:\", X_train.shape, y_train.shape)\n",
        "print(\"Validation Set Shape:\", X_val.shape, y_val.shape)\n",
        "\n",
        "# Define model parameters\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Size of the vocabulary\n",
        "embedding_dim = 128  # Dimension of the embedding layer\n",
        "max_length = X_train.shape[1]  # Maximum length of the input sequences\n",
        "num_classes = len(relation_mapping) + 1  # Number of classes including \"no relation\"\n",
        "\n",
        "# Build the LSTM model with Bidirectional LSTM for better performance\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True, kernel_regularizer=l2(0.001))))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Bidirectional(LSTM(64, kernel_regularizer=l2(0.001))))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax', kernel_regularizer=l2(0.001)))  # +1 for \"no relation\" class\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bB_olAzQ6fPH",
        "outputId": "8dc6c92d-3d56-4932-ee75-687ebda25e31"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Events DataFrame:\n",
            "  EVENT ID EVENT Text                        Context Sentence\n",
            "0       e1   watching  \\nNEW YORK _ A Brooklyn woman who was \n",
            "1       e2     killed  \\nNEW YORK _ A Brooklyn woman who was \n",
            "2       e4    emptied  \\nNEW YORK _ A Brooklyn woman who was \n",
            "3       e6       said  \\nNEW YORK _ A Brooklyn woman who was \n",
            "4       e7   appeared  \\nNEW YORK _ A Brooklyn woman who was \n",
            "\n",
            "TIMEX3 DataFrame:\n",
            "  TIMEX3 ID       TIMEX3 Text\n",
            "0       t44  Thursday evening\n",
            "1       t46  around 7:15 p.m.\n",
            "2       t47   a few years ago\n",
            "3        t1           Tuesday\n",
            "4        t2      three months\n",
            "\n",
            "T-LINKs DataFrame:\n",
            "  Event ID 1 Event ID 2      Relation\n",
            "0      ei216      ei215      INCLUDES\n",
            "1      ei239      ei238        BEFORE\n",
            "2      ei210      ei211      INCLUDES\n",
            "3      ei211      ei212  SIMULTANEOUS\n",
            "4      ei214      ei215         AFTER\n",
            "Training Set Shape: (44, 1) (44,)\n",
            "Validation Set Shape: (12, 1) (12,)\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 991ms/step - accuracy: 0.2642 - loss: 2.6376 - val_accuracy: 1.0000 - val_loss: 2.6000\n",
            "Epoch 2/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 2.5957 - val_accuracy: 1.0000 - val_loss: 2.5590\n",
            "Epoch 3/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 2.5546 - val_accuracy: 1.0000 - val_loss: 2.5187\n",
            "Epoch 4/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 1.0000 - loss: 2.5140 - val_accuracy: 1.0000 - val_loss: 2.4791\n",
            "Epoch 5/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 2.4745 - val_accuracy: 1.0000 - val_loss: 2.4399\n",
            "Epoch 6/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 1.0000 - loss: 2.4354 - val_accuracy: 1.0000 - val_loss: 2.4009\n",
            "Epoch 7/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 2.3940 - val_accuracy: 1.0000 - val_loss: 2.3619\n",
            "Epoch 8/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 2.3578 - val_accuracy: 1.0000 - val_loss: 2.3228\n",
            "Epoch 9/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 2.3163 - val_accuracy: 1.0000 - val_loss: 2.2833\n",
            "Epoch 10/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 2.2795 - val_accuracy: 1.0000 - val_loss: 2.2431\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │           \u001b[38;5;34m6,400\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_4 (\u001b[38;5;33mBidirectional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │          \u001b[38;5;34m98,816\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_5 (\u001b[38;5;33mBidirectional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m98,816\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)                   │             \u001b[38;5;34m903\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">6,400</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">903</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m614,807\u001b[0m (2.35 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">614,807</span> (2.35 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m204,935\u001b[0m (800.53 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">204,935</span> (800.53 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m409,872\u001b[0m (1.56 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">409,872</span> (1.56 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling Multiple Events"
      ],
      "metadata": {
        "id": "z6bDiJ3W8FTW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(no relation means theres no index,since we were moving away from outputting indices)\n"
      ],
      "metadata": {
        "id": "TqzFQS_58dJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example sentences with corresponding events\n",
        "example_sentences = [\n",
        "    \"John went to the store.\",\n",
        "    \"He bought milk.\",\n",
        "    \"Later, he returned home.\",\n",
        "    \"The dog barked.\",\n",
        "    \"He fed the dog.\"\n",
        "]\n",
        "\n",
        "# Corresponding events (for demonstration purposes)\n",
        "example_events = [\n",
        "    \"went to the store\",\n",
        "    \"bought milk\",\n",
        "    \"returned home\",\n",
        "    \"barked\",\n",
        "    \"fed the dog\"\n",
        "]\n",
        "\n",
        "# Tokenize and pad the example events\n",
        "example_event_sequences = tokenizer.texts_to_sequences(example_events)\n",
        "padded_example_event_sequences = pad_sequences(example_event_sequences, padding='post', maxlen=X_train.shape[1])\n",
        "\n",
        "# Make predictions using the trained model\n",
        "predictions = model.predict(padded_example_event_sequences)\n",
        "\n",
        "# Decode predictions to find the corresponding relations\n",
        "predicted_relations = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Create a mapping from index to relation\n",
        "index_to_relation = {idx: relation for relation, idx in relation_mapping.items()}\n",
        "\n",
        "# Display the predictions for each event\n",
        "for event, relation_index in zip(example_events, predicted_relations):\n",
        "    predicted_relation = index_to_relation.get(relation_index, \"No relation\")\n",
        "    print(f\"Event: '{event}' - Predicted Relation: {predicted_relation}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfB98Fzl62pb",
        "outputId": "01bc7302-11a1-4d5b-e168-8bb3a305eaa2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 564ms/step\n",
            "Event: 'went to the store' - Predicted Relation: No relation\n",
            "Event: 'bought milk' - Predicted Relation: No relation\n",
            "Event: 'returned home' - Predicted Relation: No relation\n",
            "Event: 'barked' - Predicted Relation: No relation\n",
            "Event: 'fed the dog' - Predicted Relation: No relation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compound Sentences"
      ],
      "metadata": {
        "id": "tl7fiqF47tRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example sentence with four events\n",
        "example_sentence_four_events = \"Alice woke up, made breakfast, attended a meeting, and then went for a run.\"\n",
        "\n",
        "# Corresponding events\n",
        "example_events_four = [\n",
        "    \"woke up\",\n",
        "    \"made breakfast\",\n",
        "    \"attended a meeting\",\n",
        "    \"went for a run\"\n",
        "]\n",
        "\n",
        "# Tokenize and pad the example events\n",
        "example_event_sequences_four = tokenizer.texts_to_sequences(example_events_four)\n",
        "padded_example_event_sequences_four = pad_sequences(example_event_sequences_four, padding='post', maxlen=X_train.shape[1])\n",
        "\n",
        "# Make predictions using the trained model\n",
        "predictions_four = model.predict(padded_example_event_sequences_four)\n",
        "\n",
        "# Decode predictions to find the corresponding relations\n",
        "predicted_relations_four = np.argmax(predictions_four, axis=1)\n",
        "\n",
        "# Create a mapping from index to relation\n",
        "index_to_relation = {idx: relation for relation, idx in relation_mapping.items()}\n",
        "\n",
        "# Display the predictions for each event\n",
        "for event, relation_index in zip(example_events_four, predicted_relations_four):\n",
        "    predicted_relation = index_to_relation.get(relation_index, \"No relation\")\n",
        "    print(f\"Event: '{event}' - Predicted Relation: {predicted_relation}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzONH2ZQ7qrw",
        "outputId": "e09c10dc-3351-4185-83f9-1d96ea88033c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 620ms/step\n",
            "Event: 'woke up' - Predicted Relation: No relation\n",
            "Event: 'made breakfast' - Predicted Relation: No relation\n",
            "Event: 'attended a meeting' - Predicted Relation: No relation\n",
            "Event: 'went for a run' - Predicted Relation: No relation\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}