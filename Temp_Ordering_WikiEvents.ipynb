{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# About WikiEvents:\n",
        "\n"
      ],
      "metadata": {
        "id": "Pph48YZjtnin"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6m_qbhq4ja2"
      },
      "source": [
        "# Problem Posing\n",
        "\n",
        "You want the model to read a sentence or sequence of sentences that contain multiple events and predict the chronological order in which those events occurred. The challenge is not just to extract events but to sequence them correctly based on temporal information in the text.\n",
        "\n",
        "Summary of Inputs/Outputs\n",
        "\n",
        "Input: A sentence with multiple events and temporal markers.\n",
        "\n",
        "Output: The actual events in the correct chronological order."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUCu46xdrR5g",
        "outputId": "f6dbb283-397a-47b4-9f8f-c54f1134ad79"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bx4kaAF7Uco_",
        "outputId": "4fbbe395-0cb8-460f-c6d7-65874239c860"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sentences DataFrame Attributes:\n",
            "Shape: (30000, 1)\n",
            "Columns: ['Input Sentences']\n",
            "\n",
            "Sample Data:\n",
            "                                      Input Sentences\n",
            "0  Philippines ENTITYSEP invasion of the Philippi...\n",
            "1  Battle of Mill Springs ENTITYSEP Mill Springs ...\n",
            "2  Paraguayan War ENTITYSEP Brazil ENTITYSEP Urug...\n",
            "3  Leoš Janáček ENTITYSEP The Makropulos Affair E...\n",
            "4  Lucas Oil Stadium ENTITYSEP Wisconsin ENTITYSE...\n",
            "\n",
            "Target Events DataFrame Attributes:\n",
            "Shape: (30000, 1)\n",
            "Columns: ['Target Events']\n",
            "\n",
            "Sample Data:\n",
            "                                        Target Events\n",
            "0  The first invasion of the Philippines was at B...\n",
            "1  The John D. Dingell , Jr. Conservation , Manag...\n",
            "2  The conflict between Brazil and Uruguay was se...\n",
            "3  Between 1923 and 1925 , Leoš Janáček adapted t...\n",
            "4  In the Big Ten Championship held on December 3...\n",
            "\n",
            "Padded Input Sequences DataFrame Attributes:\n",
            "Shape: (30000, 304)\n",
            "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303]\n",
            "\n",
            "Sample Data:\n",
            "    0    1    2    3    4    5    6    7    8    9    ...  294  295  296  297  \\\n",
            "0    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
            "1    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
            "2    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
            "3    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
            "4    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
            "\n",
            "   298  299  300  301  302  303  \n",
            "0    0    0    0    0    0    0  \n",
            "1    0    0    0    0    0    0  \n",
            "2    0    0    0    0    0    0  \n",
            "3    0    0    0    0    0    0  \n",
            "4    0    0    0    0    0    0  \n",
            "\n",
            "[5 rows x 304 columns]\n",
            "\n",
            "Padded Target Sequences DataFrame Attributes:\n",
            "Shape: (30000, 304)\n",
            "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303]\n",
            "\n",
            "Sample Data:\n",
            "      0      1      2      3     4     5      6    7    8    9    ...  294  \\\n",
            "0  26999      0      0      0     0     0      0    0    0    0  ...    0   \n",
            "1  19978   9271   5307  41120  2077  4418  27711    0    0    0  ...    0   \n",
            "2  35078      0      0      0     0     0      0    0    0    0  ...    0   \n",
            "3  21614  13468      0      0     0     0      0    0    0    0  ...    0   \n",
            "4  34680  40250  22190  38995     0     0      0    0    0    0  ...    0   \n",
            "\n",
            "   295  296  297  298  299  300  301  302  303  \n",
            "0    0    0    0    0    0    0    0    0    0  \n",
            "1    0    0    0    0    0    0    0    0    0  \n",
            "2    0    0    0    0    0    0    0    0    0  \n",
            "3    0    0    0    0    0    0    0    0    0  \n",
            "4    0    0    0    0    0    0    0    0    0  \n",
            "\n",
            "[5 rows x 304 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the WikiEvent Dataset\n",
        "def load_data(input_file, target_file):\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        input_sentences = f.readlines()\n",
        "\n",
        "    with open(target_file, 'r', encoding='utf-8') as f:\n",
        "        target_events = f.readlines()\n",
        "\n",
        "    return input_sentences, target_events\n",
        "\n",
        "# Tokenize the data\n",
        "def tokenize_data(input_sentences, target_events):\n",
        "    tokenized_inputs = [word_tokenize(sentence.lower()) for sentence in input_sentences]\n",
        "    tokenized_targets = [events.strip().split(',') for events in target_events]  # Comma-separated events\n",
        "\n",
        "    return tokenized_inputs, tokenized_targets\n",
        "\n",
        "# Create Mappings for Events\n",
        "def create_mappings(tokenized_targets):\n",
        "    event_set = set(event for events in tokenized_targets for event in events)\n",
        "    event_to_index = {event: i for i, event in enumerate(event_set)}\n",
        "    index_to_event = {i: event for event, i in event_to_index.items()}\n",
        "\n",
        "    return event_to_index, index_to_event\n",
        "\n",
        "# Encode Targets\n",
        "def encode_targets(tokenized_targets, event_to_index):\n",
        "    encoded_targets = [[event_to_index.get(event, 0) for event in events] for events in tokenized_targets]\n",
        "    return encoded_targets\n",
        "\n",
        "# Function to convert input sequences to indices\n",
        "def input_to_index(sequence, vocab):\n",
        "    return [vocab.get(word, 0) for word in sequence]  # 0 for unknown words\n",
        "\n",
        "# Pad Sequences\n",
        "def pad_sequences_data(tokenized_inputs, encoded_targets, vocab, event_to_index):\n",
        "    # Find the maximum length between inputs and targets\n",
        "    max_input_length = max(len(seq) for seq in tokenized_inputs)\n",
        "    max_target_length = max(len(seq) for seq in encoded_targets)\n",
        "    max_sequence_length = max(max_input_length, max_target_length)\n",
        "\n",
        "    # Pad both input and target sequences to the same max length\n",
        "    padded_inputs = pad_sequences([input_to_index(seq, vocab) for seq in tokenized_inputs],\n",
        "                                  padding='post', maxlen=max_sequence_length)\n",
        "    padded_targets = pad_sequences(encoded_targets,\n",
        "                                   padding='post', maxlen=max_sequence_length)\n",
        "\n",
        "    # Reshape padded_targets to be (num_samples, max_target_length, 1)\n",
        "    padded_targets = padded_targets[..., None]  # Add an extra dimension\n",
        "\n",
        "    return padded_inputs, padded_targets, max_sequence_length  # Return max_sequence_length\n",
        "\n",
        "# Main Function to Execute Preprocessing\n",
        "def preprocess_wikievent(input_file, target_file):\n",
        "    # Step 1: Load the data\n",
        "    input_sentences, target_events = load_data(input_file, target_file)\n",
        "\n",
        "    # Create DataFrames for raw data\n",
        "    input_df = pd.DataFrame({'Input Sentences': input_sentences})\n",
        "    target_df = pd.DataFrame({'Target Events': target_events})\n",
        "\n",
        "    # Print DataFrame attributes\n",
        "    print(\"Input Sentences DataFrame Attributes:\")\n",
        "    print(f\"Shape: {input_df.shape}\")\n",
        "    print(\"Columns:\", input_df.columns.tolist())\n",
        "    print(\"\\nSample Data:\\n\", input_df.head())\n",
        "\n",
        "    print(\"\\nTarget Events DataFrame Attributes:\")\n",
        "    print(f\"Shape: {target_df.shape}\")\n",
        "    print(\"Columns:\", target_df.columns.tolist())\n",
        "    print(\"\\nSample Data:\\n\", target_df.head())\n",
        "\n",
        "    # Step 2: Tokenization\n",
        "    tokenized_inputs, tokenized_targets = tokenize_data(input_sentences, target_events)\n",
        "\n",
        "    # Step 3: Create Mappings\n",
        "    event_to_index, index_to_event = create_mappings(tokenized_targets)\n",
        "\n",
        "    # Step 4: Encode Targets\n",
        "    encoded_targets = encode_targets(tokenized_targets, event_to_index)\n",
        "\n",
        "    # Step 5: Padding\n",
        "    padded_inputs, padded_targets, max_sequence_length = pad_sequences_data(\n",
        "        tokenized_inputs, encoded_targets, event_to_index, event_to_index\n",
        "    )\n",
        "\n",
        "    # Create DataFrames for padded inputs and targets\n",
        "    padded_inputs_df = pd.DataFrame(padded_inputs)\n",
        "    padded_targets_flat = padded_targets.reshape(padded_targets.shape[0], padded_targets.shape[1])\n",
        "    padded_targets_df = pd.DataFrame(padded_targets_flat)\n",
        "\n",
        "    # Print DataFrame attributes for padded inputs and targets\n",
        "    print(\"\\nPadded Input Sequences DataFrame Attributes:\")\n",
        "    print(f\"Shape: {padded_inputs_df.shape}\")\n",
        "    print(\"Columns:\", padded_inputs_df.columns.tolist())\n",
        "    print(\"\\nSample Data:\\n\", padded_inputs_df.head())\n",
        "\n",
        "    print(\"\\nPadded Target Sequences DataFrame Attributes:\")\n",
        "    print(f\"Shape: {padded_targets_df.shape}\")\n",
        "    print(\"Columns:\", padded_targets_df.columns.tolist())\n",
        "    print(\"\\nSample Data:\\n\", padded_targets_df.head())\n",
        "\n",
        "    return max_sequence_length, event_to_index  # Return necessary values for the next steps\n",
        "\n",
        "# Example Usage\n",
        "input_file = 'wikievent.src'  # Change to your actual input file path\n",
        "target_file = 'wikievent.tgt'  # Change to your actual target file path\n",
        "\n",
        "max_sequence_length, event_to_index = preprocess_wikievent(input_file, target_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuill-o6ijYU",
        "outputId": "5f1744ad-6764-48d2-db2a-036830f1f9e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m 38/750\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:01:01\u001b[0m 25s/step - accuracy: 0.8828 - loss: 9.2158"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the WikiEvent Dataset\n",
        "def load_data(input_file, target_file):\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        input_sentences = f.readlines()\n",
        "\n",
        "    with open(target_file, 'r', encoding='utf-8') as f:\n",
        "        target_events = f.readlines()\n",
        "\n",
        "    return input_sentences, target_events\n",
        "\n",
        "# Tokenize the data\n",
        "def tokenize_data(input_sentences, target_events):\n",
        "    tokenized_inputs = [word_tokenize(sentence.lower()) for sentence in input_sentences]\n",
        "    tokenized_targets = [events.strip().split(',') for events in target_events]  # Comma-separated events\n",
        "\n",
        "    return tokenized_inputs, tokenized_targets\n",
        "\n",
        "# Create Mappings for Events\n",
        "def create_mappings(tokenized_targets):\n",
        "    event_set = set(event for events in tokenized_targets for event in events)\n",
        "    event_to_index = {event: i for i, event in enumerate(event_set)}\n",
        "    index_to_event = {i: event for event, i in event_to_index.items()}\n",
        "\n",
        "    return event_to_index, index_to_event\n",
        "\n",
        "# Encode Targets\n",
        "def encode_targets(tokenized_targets, event_to_index):\n",
        "    encoded_targets = [[event_to_index.get(event, 0) for event in events] for events in tokenized_targets]\n",
        "    return encoded_targets\n",
        "\n",
        "# Function to convert input sequences to indices\n",
        "def input_to_index(sequence, vocab):\n",
        "    return [vocab.get(word, 0) for word in sequence]  # 0 for unknown words\n",
        "\n",
        "# Pad Sequences\n",
        "def pad_sequences_data(tokenized_inputs, encoded_targets, vocab):\n",
        "    # Find the maximum length between inputs and targets\n",
        "    max_input_length = max(len(seq) for seq in tokenized_inputs)\n",
        "    max_target_length = max(len(seq) for seq in encoded_targets)\n",
        "    max_sequence_length = max(max_input_length, max_target_length)\n",
        "\n",
        "    # Pad both input and target sequences to the same max length\n",
        "    padded_inputs = pad_sequences([input_to_index(seq, vocab) for seq in tokenized_inputs],\n",
        "                                  padding='post', maxlen=max_sequence_length)\n",
        "    padded_targets = pad_sequences(encoded_targets,\n",
        "                                   padding='post', maxlen=max_sequence_length)\n",
        "\n",
        "    # Reshape padded_targets to be (num_samples, max_target_length, 1)\n",
        "    padded_targets = padded_targets[..., None]  # Add an extra dimension\n",
        "\n",
        "    return padded_inputs, padded_targets, max_sequence_length  # Return max_sequence_length\n",
        "\n",
        "# Main Function to Execute Preprocessing\n",
        "def preprocess_wikievent(input_file, target_file):\n",
        "    # Step 1: Load the data\n",
        "    input_sentences, target_events = load_data(input_file, target_file)\n",
        "\n",
        "    # Step 2: Tokenization\n",
        "    tokenized_inputs, tokenized_targets = tokenize_data(input_sentences, target_events)\n",
        "\n",
        "    # Step 3: Create Mappings\n",
        "    event_to_index, index_to_event = create_mappings(tokenized_targets)\n",
        "\n",
        "    # Step 4: Encode Targets\n",
        "    encoded_targets = encode_targets(tokenized_targets, event_to_index)\n",
        "\n",
        "    # Step 5: Padding\n",
        "    padded_inputs, padded_targets, max_sequence_length = pad_sequences_data(\n",
        "        tokenized_inputs, encoded_targets, event_to_index\n",
        "    )\n",
        "\n",
        "    return padded_inputs, padded_targets, max_sequence_length, event_to_index  # Return necessary values for the next steps\n",
        "\n",
        "# Example Usage\n",
        "input_file = 'wikievent.src'  # Change to your actual input file path\n",
        "target_file = 'wikievent.tgt'  # Change to your actual target file path\n",
        "\n",
        "# Preprocess the WikiEvent data\n",
        "padded_inputs, padded_targets, max_sequence_length, event_to_index = preprocess_wikievent(input_file, target_file)\n",
        "\n",
        "# Split the dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_inputs, padded_targets, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and compile the model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "\n",
        "def create_model(vocab_size, embedding_dim, max_sequence_length):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "    model.add(LSTM(128, return_sequences=True))\n",
        "    model.add(Dense(vocab_size, activation='softmax'))  # Assuming you want to output probabilities for each event\n",
        "\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train the model\n",
        "model = create_model(len(event_to_index), embedding_dim=64, max_sequence_length=max_sequence_length)\n",
        "\n",
        "# Fit the model with the correct target shape\n",
        "history = model.fit(X_train, y_train.reshape(y_train.shape[0], y_train.shape[1]),\n",
        "                    validation_data=(X_test, y_test.reshape(y_test.shape[0], y_test.shape[1])),\n",
        "                    epochs=10, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict and return the temporal order of events (including compound events)\n",
        "def predict_temporal_order(input_sentence, model, vocab, index_to_event, max_sequence_length):\n",
        "    # Tokenize and pad the input sentence\n",
        "    tokenized_input = word_tokenize(input_sentence.lower())\n",
        "    input_indices = pad_sequences([input_to_index(tokenized_input, vocab)], maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "    # Predict the output sequence\n",
        "    predictions = model.predict(input_indices)\n",
        "    predicted_indices = predictions.argmax(axis=-1)[0]  # Get the index with the highest probability\n",
        "\n",
        "    # Convert indices back to events\n",
        "    predicted_events = [index_to_event.get(idx, '<UNK>') for idx in predicted_indices if idx != 0]  # Skip padding (0)\n",
        "\n",
        "    return predicted_events\n"
      ],
      "metadata": {
        "id": "60NCNkt6teMD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example sentence with compound events\n",
        "input_sentence = \"The president arrived at the meeting, gave a speech, and then left the venue.\"\n",
        "\n",
        "# Predict the temporal order of events\n",
        "predicted_events = predict_temporal_order(input_sentence, model, event_to_index, index_to_event, max_sequence_length)\n",
        "\n",
        "print(\"Predicted Temporal Order of Events:\", predicted_events)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "o84mBrKLtgMw",
        "outputId": "f3f3ffd3-53f1-4669-b319-f1aa4113e492"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-76bd146682b7>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Predict the temporal order of events\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpredicted_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_temporal_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_to_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_to_event\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicted Temporal Order of Events:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_events\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}