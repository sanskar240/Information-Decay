{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Function to parse a TML file (TimeML format) and extract events, T-LINKs, and TIMEX3\n",
        "def parse_tml_with_context(file_path):\n",
        "    tree = ET.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    events = []\n",
        "    tlinks = []\n",
        "    timex3s = []\n",
        "\n",
        "    # Extract events and TIMEX3\n",
        "    for s in root.iter('TEXT'):\n",
        "        sentence_text = s.text\n",
        "        for event in s.iter('EVENT'):\n",
        "            event_id = event.attrib['eid']\n",
        "            event_text = event.text\n",
        "            events.append({'EVENT ID': event_id, 'EVENT Text': event_text, 'Context Sentence': sentence_text})\n",
        "\n",
        "        for timex in s.iter('TIMEX3'):\n",
        "            timex_id = timex.attrib['tid']\n",
        "            timex_text = timex.text\n",
        "            timex3s.append({'TIMEX3 ID': timex_id, 'TIMEX3 Text': timex_text})\n",
        "\n",
        "    # Extract T-LINKs\n",
        "    for tlink in root.iter('TLINK'):\n",
        "        event_id_1 = tlink.attrib.get('eventInstanceID')\n",
        "        event_id_2 = tlink.attrib.get('relatedToEventInstance')\n",
        "        relation = tlink.attrib.get('relType')\n",
        "\n",
        "        if event_id_1 and event_id_2:\n",
        "            tlinks.append({'Event ID 1': event_id_1, 'Event ID 2': event_id_2, 'Relation': relation})\n",
        "\n",
        "    events_df = pd.DataFrame(events)\n",
        "    timex3_df = pd.DataFrame(timex3s)\n",
        "    tlinks_df = pd.DataFrame(tlinks)\n",
        "\n",
        "    return events_df, timex3_df, tlinks_df\n",
        "\n",
        "# Load the datasets\n",
        "timebank_events_df, timebank_timex3_df, timebank_tlinks_df = parse_tml_with_context('TimeBank.tml')\n",
        "timeeval3_events_df, timeeval3_timex3_df, timeeval3_tlinks_df = parse_tml_with_context('TimeEval3.tml')\n",
        "\n",
        "# Combine the datasets\n",
        "combined_events_df = pd.concat([timebank_events_df, timeeval3_events_df], ignore_index=True)\n",
        "combined_timex3_df = pd.concat([timebank_timex3_df, timeeval3_timex3_df], ignore_index=True)\n",
        "combined_tlinks_df = pd.concat([timebank_tlinks_df, timeeval3_tlinks_df], ignore_index=True)\n",
        "\n",
        "# Tokenization and padding\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        # Tokenize and pad sequences\n",
        "        tokenized_text = self.tokenizer(text)\n",
        "        # Ensure padding length matches max_length\n",
        "        if len(tokenized_text) < self.max_length:\n",
        "            padded_text = np.pad(tokenized_text, (0, self.max_length - len(tokenized_text)), 'constant')\n",
        "        else:\n",
        "            padded_text = tokenized_text[:self.max_length]\n",
        "        return torch.tensor(padded_text, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# Prepare input data for events\n",
        "class Tokenizer:\n",
        "    def __init__(self):\n",
        "        self.word_index = {}\n",
        "        self.index_word = {}\n",
        "\n",
        "    def fit_on_texts(self, texts):\n",
        "        words = set()\n",
        "        for text in texts:\n",
        "            for word in text.split():\n",
        "                words.add(word)\n",
        "\n",
        "        self.word_index = {word: i + 1 for i, word in enumerate(words)}\n",
        "        self.index_word = {i + 1: word for i, word in enumerate(words)}\n",
        "\n",
        "    def texts_to_sequences(self, texts):\n",
        "        sequences = []\n",
        "        for text in texts:\n",
        "            sequence = [self.word_index.get(word, 0) for word in text.split()]\n",
        "            sequences.append(sequence)\n",
        "        return sequences\n",
        "\n",
        "# Initialize and fit the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(combined_events_df['EVENT Text'].tolist())\n",
        "event_sequences = tokenizer.texts_to_sequences(combined_events_df['EVENT Text'].tolist())\n",
        "\n",
        "# Create encoded labels\n",
        "relation_mapping = {relation: idx for idx, relation in enumerate(combined_tlinks_df['Relation'].unique())}\n",
        "encoded_labels = []\n",
        "for index, row in combined_events_df.iterrows():\n",
        "    relation = combined_tlinks_df[\n",
        "        (combined_tlinks_df['Event ID 1'] == row['EVENT ID']) |\n",
        "        (combined_tlinks_df['Event ID 2'] == row['EVENT ID'])\n",
        "    ]['Relation']\n",
        "\n",
        "    if not relation.empty:\n",
        "        encoded_labels.append(relation_mapping[relation.values[0]])\n",
        "    else:\n",
        "        encoded_labels.append(len(relation_mapping))  # Adjust if needed\n",
        "\n",
        "# Convert to a numpy array\n",
        "encoded_labels = np.array(encoded_labels)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(combined_events_df['EVENT Text'].tolist(), encoded_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create datasets\n",
        "max_length = max(len(seq) for seq in event_sequences)  # Calculate maximum sequence length\n",
        "train_dataset = TextDataset(X_train, y_train, tokenizer=tokenizer.texts_to_sequences, max_length=max_length)\n",
        "val_dataset = TextDataset(X_val, y_val, tokenizer=tokenizer.texts_to_sequences, max_length=max_length)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the LSTM model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x[:, -1, :])  # Get the last time step output\n",
        "        return x\n",
        "\n",
        "# Define model parameters\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Size of the vocabulary\n",
        "embedding_dim = 128  # Dimension of the embedding layer\n",
        "hidden_dim = 64  # Number of hidden units\n",
        "num_classes = len(relation_mapping) + 1  # Number of classes including \"no relation\"\n",
        "\n",
        "# Create model\n",
        "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, num_classes)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for texts, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Reshape the input to be 2D: (batch_size, sequence_length)\n",
        "        outputs = model(texts.view(texts.size(0), -1))  # Flatten to ensure it is 2D\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in val_loader:\n",
        "            outputs = model(texts.view(texts.size(0), -1))  # Ensure it is 2D\n",
        "            val_loss += criterion(outputs, labels).item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss/len(val_loader):.4f}')\n",
        "\n",
        "# Function to order events based on model predictions\n",
        "def predict_event_order(sentence, model, tokenizer, maxlen):\n",
        "    events = sentence.split(\", \")  # Simple event extraction from the input sentence\n",
        "    event_sequences = tokenizer.texts_to_sequences(events)\n",
        "\n",
        "    # Prepare padded sequences\n",
        "    padded_sequences = np.array([np.pad(seq, (0, maxlen - len(seq)), 'constant') if len(seq) < maxlen else seq[:maxlen] for seq in event_sequences])\n",
        "    with torch.no_grad():\n",
        "        predictions = model(torch.tensor(padded_sequences, dtype=torch.long))\n",
        "\n",
        "    # Get the predicted order by selecting the highest probability for each event\n",
        "    predicted_order = np.argsort(np.argmax(predictions.numpy(), axis=1))\n",
        "\n",
        "    # Return events in the predicted order\n",
        "    ordered_events = [events[i] for i in predicted_order]\n",
        "    return ordered_events\n",
        "\n",
        "# Define a sample sentence with unordered events\n",
        "sample_sentence = \"Alice went for a meeting, woke up, made breakfast, attended a meeting, and then went for a run.\"\n",
        "\n",
        "# Predict and print the ordered events\n",
        "ordered_events = predict_event_order(sample_sentence, model, tokenizer, max_length)\n",
        "\n",
        "print(\"\\nOriginal Events:\")\n",
        "print(sample_sentence)\n",
        "\n",
        "print(\"\\nPredicted Events in Correct Temporal Order:\")\n",
        "print(ordered_events)\n"
      ],
      "metadata": {
        "id": "-VNfDQWK0kZR",
        "outputId": "ff795c42-3637-49b3-b2ee-d8780249a532",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.6779, Val Loss: 1.4991\n",
            "Epoch [2/10], Loss: 1.3449, Val Loss: 1.2096\n",
            "Epoch [3/10], Loss: 1.0538, Val Loss: 0.9443\n",
            "Epoch [4/10], Loss: 0.8205, Val Loss: 0.7113\n",
            "Epoch [5/10], Loss: 0.6195, Val Loss: 0.5176\n",
            "Epoch [6/10], Loss: 0.5162, Val Loss: 0.3657\n",
            "Epoch [7/10], Loss: 0.3279, Val Loss: 0.2533\n",
            "Epoch [8/10], Loss: 0.2390, Val Loss: 0.1739\n",
            "Epoch [9/10], Loss: 0.1541, Val Loss: 0.1198\n",
            "Epoch [10/10], Loss: 0.1051, Val Loss: 0.0839\n",
            "\n",
            "Original Events:\n",
            "Alice went for a meeting, woke up, made breakfast, attended a meeting, and then went for a run.\n",
            "\n",
            "Predicted Events in Correct Temporal Order:\n",
            "['Alice went for a meeting', 'woke up', 'made breakfast', 'attended a meeting', 'and then went for a run.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# After validation step\n",
        "model.eval()\n",
        "val_predictions = []\n",
        "val_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for texts, labels in val_loader:\n",
        "        outputs = model(texts.view(texts.size(0), -1))\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        val_predictions.extend(predicted.numpy())\n",
        "        val_labels.extend(labels.numpy())\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(val_labels, val_predictions)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(val_labels, val_predictions, average='weighted')\n",
        "\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXautC1Axhou",
        "outputId": "43426a67-9902-475f-91c4-c3e16f2feeb3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0000\n",
            "Precision: 1.0000\n",
            "Recall: 1.0000\n",
            "F1 Score: 1.0000\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}